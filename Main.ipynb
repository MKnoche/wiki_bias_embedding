{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from weat import WEAT, WEATvec, weat_analysis\n",
    "from sets import sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load a single embedding and convert it to a dictionary which maps words to their respective vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings/conservapedia/0.pkl', 'rb') as f:\n",
    "    embedding = pickle.load(f)\n",
    "word2vec = {word: np.array(vec) for word, vec in embedding}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Each vector has a length of 168."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.22601859e-01,  -3.01443249e-01,   6.05218530e-01,\n",
       "        -9.78568941e-02,   6.26489744e-02,  -3.68599296e-01,\n",
       "        -2.60574579e-01,  -4.04397190e-01,  -2.21888646e-01,\n",
       "        -1.02070875e-01,   1.19224519e-01,   9.26395238e-04,\n",
       "        -3.63263525e-02,  -3.57526153e-01,   3.62686425e-01,\n",
       "         5.24161197e-02,   1.76762283e-01,   5.05260587e-01,\n",
       "         6.82055950e-04,   1.54380165e-02,   4.16435689e-01,\n",
       "        -3.76806319e-01,  -2.22768441e-01,   5.43065250e-01,\n",
       "        -7.97214955e-02,   1.20553926e-01,  -1.60782903e-01,\n",
       "         1.15703523e+00,  -1.70429498e-01,   3.64926495e-02,\n",
       "         1.88335180e-02,   2.98126542e-04,   1.13126278e-01,\n",
       "        -3.11881006e-01,  -2.63291806e-01,  -3.21932822e-01,\n",
       "        -3.76520127e-01,  -5.22700846e-01,   6.57714456e-02,\n",
       "         1.65825546e-01,  -2.95625627e-01,   2.75215626e-01,\n",
       "        -3.32018316e-01,  -3.83091182e-01,  -3.40342999e-01,\n",
       "        -2.75427043e-01,  -2.65808016e-01,   3.74817774e-02,\n",
       "         5.34076989e-03,   7.78037757e-02,  -1.23823881e-01,\n",
       "         3.25096965e-01,  -9.10646990e-02,   1.40420467e-01,\n",
       "        -1.06419042e-01,  -5.06877840e-01,  -3.82172316e-01,\n",
       "         1.25867993e-01,  -1.05043910e-01,  -2.69787222e-01,\n",
       "        -5.98979115e-01,  -1.78962514e-01,  -7.06029162e-02,\n",
       "         2.72885293e-01,  -1.54033870e-01,  -1.76071301e-01,\n",
       "         3.81487072e-01,   4.34051573e-01,   1.07504405e-01,\n",
       "        -2.79350221e-01,   4.98276144e-01,  -5.14957495e-02,\n",
       "        -1.67057887e-01,  -8.56686458e-02,   1.60812110e-01,\n",
       "        -9.46450233e-02,  -1.86799303e-01,   2.09340200e-01,\n",
       "        -4.46949452e-01,  -1.79231483e-02,  -4.72140133e-01,\n",
       "        -1.44434631e-01,  -3.14068675e-01,   1.60858542e-01,\n",
       "         1.60367221e-01,  -1.49114892e-01,   7.78919235e-02,\n",
       "         2.40341365e-01,   6.32599071e-02,   6.53059706e-02,\n",
       "         3.48090470e-01,   6.55900538e-01,  -1.58505246e-01,\n",
       "        -7.00555563e-01,   7.71512985e-02,  -3.16555470e-01,\n",
       "         9.91131365e-02,  -8.57635662e-02,  -4.79666382e-01,\n",
       "         5.15963621e-02,  -7.23631158e-02,  -1.24913342e-01,\n",
       "         3.74078840e-01,   8.66607428e-02,   5.96663177e-01,\n",
       "        -5.78433610e-02,   1.93896428e-01,   2.42339775e-01,\n",
       "        -3.67538124e-01,  -2.44227111e-01,  -2.37541288e-01,\n",
       "         1.22124039e-01,   8.62514134e-03,  -1.38222083e-01,\n",
       "        -6.14014089e-01,   4.55075651e-02,  -1.81165617e-02,\n",
       "         5.54124296e-01,  -7.24329710e-01,  -1.16654076e-01,\n",
       "         2.16130883e-01,  -6.60878839e-03,   1.66068301e-01,\n",
       "         3.38489056e-01,  -3.38937968e-01,  -8.80155027e-01,\n",
       "        -2.63283223e-01,   5.69643438e-01,  -6.60726488e-01,\n",
       "        -2.84922630e-01,  -1.54362246e-01,   5.85476220e-01,\n",
       "        -2.21561223e-01,   1.15090653e-01,   6.25566363e-01,\n",
       "         5.50910160e-02,   1.55238464e-01,   1.01219617e-01,\n",
       "        -1.85786277e-01,   2.88508713e-01,   2.59620041e-01,\n",
       "        -2.03767076e-01,   1.86735600e-01,  -1.49137914e-01,\n",
       "         4.53462720e-01,   3.70270796e-02,   5.43006063e-01,\n",
       "        -4.39121187e-01,  -2.56051384e-02,  -5.07784843e-01,\n",
       "        -1.54239044e-01,  -1.00142881e-01,  -2.64498323e-01,\n",
       "         1.19116880e-01,   4.13420051e-01,   4.20489386e-02,\n",
       "        -8.13893043e-03,   1.92506090e-01,  -3.67770702e-01,\n",
       "        -3.49146545e-01,  -2.17150047e-01,  -2.59792179e-01,\n",
       "        -5.16180098e-01,   2.41971999e-01,   1.71018392e-01,\n",
       "        -4.00228024e-01,   3.35327029e-01,   3.82318050e-01], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec['apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets\n",
    "\n",
    "The file `sets.py` contains some predefined sets of words. As a first example use the attribute sets `male` and `female` together with the target sets `career` and `family`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', 'man', 'boy', 'brother', 'he']\n",
      "['female', 'woman', 'girl', 'sister', 'she']\n",
      "['executive', 'management', 'professional', 'corporation', 'salary']\n",
      "['home', 'parents', 'children', 'family', 'cousins']\n"
     ]
    }
   ],
   "source": [
    "print(sets['male'][:5])\n",
    "print(sets['female'][:5])\n",
    "print(sets['career'][:5])\n",
    "print(sets['family'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEAT\n",
    "\n",
    "The `WEAT`-object implements the Word Embedding Association Test [1]. It investigates the following null hypothesis:\n",
    "\n",
    ">The relative association of the target word sets $X$ (e.g. career) and $Y$ (e.g. family)  \n",
    ">to the attribute word sets $A$ (e.g. male) and $B$ (e.g. female) is the same.\n",
    "\n",
    "A test statistic is computed by comparing the angles between different words in those sets. For details see the referenced paper. In order to get a $p$-value, a permutation test is applied: The words of $X \\cup Y$ are randomly split into new sets $X'$ and $Y'$. Then the test statistic is computed for the new sets as well. By comparing the result of the original sets to different shuffled ones, it can be measured how “extreme” the real result is. The parameter `steps` controls the number of shufflings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat = WEAT(word2vec, sets['career'], sets['family'], sets['male'], sets['female'], steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting $p$-value and an effect size based on Cohen’s $d$ can be received using `get_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001, 2.4064634)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weat.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEATvec\n",
    "\n",
    "Instead of comparing the angles between different words, we proposed WEATvec which compares the projections of vectors onto each other. See our paper for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatvec = WEATvec(word2vec, sets['career'], sets['family'], sets['male'], sets['female'], steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001, 2.2007706)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatvec.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full analysis\n",
    "For directly running multiple tests on different wikis with multiple embeddings, the function `weat_analysis` can be utilized. To use it, firstly need to load the other embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_word2vecs = {}\n",
    "for wiki in os.listdir('embeddings'):\n",
    "    combined_word2vecs[wiki] = []\n",
    "    for pkl in os.listdir(f'embeddings/{wiki}'):\n",
    "        with open(f'embeddings/{wiki}/{pkl}', 'rb') as f:\n",
    "            embedding = pickle.load(f)        \n",
    "        word2vec = {word: np.array(vec) for word, vec in embedding}\n",
    "        combined_word2vecs[wiki].append(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `sets` only contains words which exist in all embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = [set(combined_word2vecs[wiki][0].keys()) for wiki in combined_word2vecs]\n",
    "vocab = set(vocabs[0])\n",
    "for v in vocabs[1:]:\n",
    "    vocab &= set(v)\n",
    "for s in sets:\n",
    "    in_vocab = []\n",
    "    for word in sets[s]:\n",
    "        if word in vocab:\n",
    "            in_vocab.append(word)\n",
    "    sets[s] = in_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a list of quadruples `[(A, B, X, Y)]` containing the sets which should be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadruples = [\n",
    "    ('career', 'family', 'male', 'female'),\n",
    "    ('science', 'art', 'male', 'female'),\n",
    "    ('pleasant', 'unpleasant', 'white_names', 'black_names'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `weat_analysis` can now be used to run WEAT and WEATvec. It returns a pandas dataframe. In our paper we set `steps` to _1000000_.\n",
    "Note: a Bonferroni correction is not applied automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take some time\n",
      "Finished 25 of 72\n",
      "Finished 50 of 72\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "res = weat_analysis(combined_word2vecs, quadruples, sets, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p$-values are combined using Fisher’s method, for the effect size $d$ the mean is returned. In the result the columns `fishers_p` and `mean_d` correspond to WEAT, `fishers_p2` and `mean_d2` to WEATvec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>wiki</th>\n",
       "      <th>fishers_p</th>\n",
       "      <th>mean_d</th>\n",
       "      <th>fishers_p2</th>\n",
       "      <th>mean_d2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(career, family, male, female)</td>\n",
       "      <td>conservapedia</td>\n",
       "      <td>6.539677e-16</td>\n",
       "      <td>2.419994</td>\n",
       "      <td>3.565607e-16</td>\n",
       "      <td>2.283538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(career, family, male, female)</td>\n",
       "      <td>rationalwiki</td>\n",
       "      <td>1.040249e-14</td>\n",
       "      <td>1.734437</td>\n",
       "      <td>1.525946e-13</td>\n",
       "      <td>1.709633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(career, family, male, female)</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>1.198144e-15</td>\n",
       "      <td>2.432250</td>\n",
       "      <td>3.565607e-16</td>\n",
       "      <td>2.320894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(science, art, male, female)</td>\n",
       "      <td>conservapedia</td>\n",
       "      <td>1.477972e-14</td>\n",
       "      <td>1.374478</td>\n",
       "      <td>6.919962e-15</td>\n",
       "      <td>1.405803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(science, art, male, female)</td>\n",
       "      <td>rationalwiki</td>\n",
       "      <td>5.651508e-11</td>\n",
       "      <td>0.823628</td>\n",
       "      <td>1.532005e-11</td>\n",
       "      <td>0.888437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(science, art, male, female)</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>2.192705e-15</td>\n",
       "      <td>1.634619</td>\n",
       "      <td>1.198144e-15</td>\n",
       "      <td>1.545134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(pleasant, unpleasant, white_names, black_names)</td>\n",
       "      <td>conservapedia</td>\n",
       "      <td>2.971393e-10</td>\n",
       "      <td>0.666569</td>\n",
       "      <td>1.498999e-09</td>\n",
       "      <td>0.587838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(pleasant, unpleasant, white_names, black_names)</td>\n",
       "      <td>rationalwiki</td>\n",
       "      <td>1.382411e-01</td>\n",
       "      <td>0.149103</td>\n",
       "      <td>1.365359e-01</td>\n",
       "      <td>0.137169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(pleasant, unpleasant, white_names, black_names)</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>1.090513e-05</td>\n",
       "      <td>0.357216</td>\n",
       "      <td>6.633838e-05</td>\n",
       "      <td>0.268057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              names           wiki  \\\n",
       "0                    (career, family, male, female)  conservapedia   \n",
       "1                    (career, family, male, female)   rationalwiki   \n",
       "2                    (career, family, male, female)      wikipedia   \n",
       "3                      (science, art, male, female)  conservapedia   \n",
       "4                      (science, art, male, female)   rationalwiki   \n",
       "5                      (science, art, male, female)      wikipedia   \n",
       "6  (pleasant, unpleasant, white_names, black_names)  conservapedia   \n",
       "7  (pleasant, unpleasant, white_names, black_names)   rationalwiki   \n",
       "8  (pleasant, unpleasant, white_names, black_names)      wikipedia   \n",
       "\n",
       "      fishers_p    mean_d    fishers_p2   mean_d2  \n",
       "0  6.539677e-16  2.419994  3.565607e-16  2.283538  \n",
       "1  1.040249e-14  1.734437  1.525946e-13  1.709633  \n",
       "2  1.198144e-15  2.432250  3.565607e-16  2.320894  \n",
       "3  1.477972e-14  1.374478  6.919962e-15  1.405803  \n",
       "4  5.651508e-11  0.823628  1.532005e-11  0.888437  \n",
       "5  2.192705e-15  1.634619  1.198144e-15  1.545134  \n",
       "6  2.971393e-10  0.666569  1.498999e-09  0.587838  \n",
       "7  1.382411e-01  0.149103  1.365359e-01  0.137169  \n",
       "8  1.090513e-05  0.357216  6.633838e-05  0.268057  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[['names', 'wiki', 'fishers_p', 'mean_d', 'fishers_p2', 'mean_d2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "[1] Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. \"Semantics derived automatically from language corpora contain human-like biases.\" Science 356.6334 (2017): 183-186."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
